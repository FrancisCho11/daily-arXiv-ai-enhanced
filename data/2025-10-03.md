<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO是首个基于视频一致性模型的零样本视频恢复求解器，通过利用VCMs的时序因果性，在保持测量一致性的同时实现高质量视频重建，显著优于逐帧应用图像LDM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像扩散模型的零样本逆求解器在视频恢复中面临挑战，逐帧应用会导致时序不一致问题，需要能够同时恢复空间细节和捕捉时序依赖的新方法。

Method: 利用视频一致性模型(VCMs)作为先验，构建零样本视频逆求解器LVTINO，通过条件机制绕过自动微分需求，仅需少量神经网络评估即可实现高质量重建。

Result: 在多种视频逆问题上的实验表明，LVTINO在重建保真度和计算效率方面均优于当前基于图像LDM的逐帧方法，实现了显著的感知质量提升。

Conclusion: LVTINO通过视频一致性模型成功解决了视频恢复中的时序一致性问题，为高分辨率视频逆问题提供了新的基准方法。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出了一种基于风格提取的三阶段图像生成方法，通过风格编码器和投影层将风格表示与文本表示对齐，实现细粒度文本引导的风格化图像生成。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中细粒度风格难以用自然语言精确描述和控制的问题，以及风格参考图像的指导信息难以与文本条件直接对齐的挑战。

Method: 使用风格编码器和风格投影层从单一风格参考图像中提取细粒度风格表示，在不改变下游生成模型结构框架的情况下注入风格表示。构建了包含图像、风格标签和文本描述三元组的Style30k-captions数据集。

Result: 实现了基于文本提示的细粒度风格引导生成，能够精确控制生成图像的风格特征。

Conclusion: 该方法能够最大化预训练生成模型的生成能力，通过风格表示与文本表示的对齐，实现细粒度可控的风格化图像生成。

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 该研究收集了一个包含61.68小时视频、2,793个视频片段和5,385个标注时间段的挣扎检测数据集，用于研究技能学习过程中挣扎的演变。


<details>
  <summary>Details</summary>
Motivation: 现有操作数据集未关注挣扎随时间的演变，而理解这种演变对于确定用户当前学习阶段和开发有效辅助系统至关重要。

Method: 收集了76名参与者在18个任务（分为四种活动：打结、折纸、七巧板、洗牌）的数据，每个任务重复五次以捕捉技能演变。将挣扎检测定义为时间动作定位任务。

Result: 时间动作定位模型能够成功检测挣扎线索，在未见任务和活动上的平均mAP分别为34.56%和19.24%，表明挣扎是跨任务可迁移的概念。

Conclusion: 挣扎是跨各种基于技能任务的可迁移概念，但在挣扎检测方面仍需进一步改进。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS是一个紧凑高效的偏微分方程求解基础模型，使用轻量级残差U-Net架构，通过自回归预训练策略学习物理规律，在多种下游PDE任务上实现最先进的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型主要基于大型复杂Transformer架构，计算和参数开销大。SPUS旨在探索轻量级U-Net架构作为基础模型的潜力，提供更高效的解决方案。

Method: 采用轻量级残差U-Net架构，使用自回归预训练策略模拟数值求解器行为，在多样化流体动力学PDE数据集上进行预训练。

Result: 在6个未见过的下游PDE任务上达到最先进的泛化性能，同时需要显著更少的参数和微调数据。

Conclusion: SPUS展示了残差U-Net架构作为参数高效PDE求解基础模型的巨大潜力，为求解多样化PDE系统提供了更轻量级的解决方案。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo是一个基于强化学习的框架，通过多样性约束直接优化多人生成中的身份多样性，解决了现有文本到图像模型在多人提示下重复面孔、合并身份和计数错误的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到图像模型在现实主义方面表现出色，但在处理多人生成提示时存在严重问题，包括重复面孔、身份合并和个体计数错误。

Method: 使用Group-Relative Policy Optimization (GRPO)对流匹配模型进行微调，采用组合奖励函数：(i)惩罚图像内面部相似性，(ii)阻止跨样本身份重复，(iii)确保准确的人物计数，(iv)通过人类偏好分数保持视觉保真度。采用单阶段课程学习稳定训练。

Result: 在DiverseHumans测试集上，DisCo实现了98.6%的唯一面孔准确率和接近完美的全局身份分布，超越了开源和专有方法（如Gemini、GPT-Image），同时保持竞争力的感知质量。

Conclusion: DisCo作为一个可扩展、无需额外标注的解决方案，解决了生成模型中长期存在的身份危机问题，为组合式多人生成设立了新的基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 提出了一种新的视觉地理定位方法，通过将查询图像的视觉表示与分层地理嵌入对齐，并结合外观特征和语义分割图，在多个基准数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 全球视觉地理定位旨在仅使用图像视觉内容确定其地理位置，现有方法在学习地理表示方面仍有改进空间。

Method: 将地理定位建模为视觉表示与学习的地理表示对齐，使用分层地理嵌入表示世界，并融合外观特征和语义分割图形成鲁棒的视觉表示。

Result: 在5个基准数据集的25个指标中，22个指标达到了历史最佳性能，超越了现有最先进方法和大型视觉语言模型。

Conclusion: 性能提升主要归功于地理表示和视觉表示的有效结合，证明了该方法在视觉地理定位任务中的有效性。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS是首个用于大型视觉语言模型数据高效指令调优的原则性方法，通过基于跨模态注意力矩阵相似性聚类训练样本，可大幅减少训练数据量而不损失性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法在大型视觉语言模型上表现不佳，无法超越随机选择。需要开发专门针对LVLM的数据高效学习方法。

Method: 证明具有相似跨模态注意力矩阵的样本具有相似梯度，基于此提出XMAS方法：使用代理LVLM微调获取注意力矩阵的顶部奇异值轨迹，聚类样本并从中平衡采样。

Result: XMAS可丢弃LLaVA-665k数据集的50%和Vision-Flan数据集的85%，完全保持LLaVA-1.5-7B在10个下游基准上的性能，训练速度提升1.2倍。

Conclusion: XMAS是首个在LVLM指令调优中超越随机选择的数据选择方法，能有效消除大规模训练数据中的冗余，实现数据高效学习。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种变分流匹配方法，用于向量量化图像生成，结合了连续传输动态和明确的分类监督。


<details>
  <summary>Details</summary>
Motivation: 传统方法在连续方法和分类方法之间存在权衡，Purrception旨在结合两者的优势：连续方法的几何感知能力和分类方法的离散监督能力。

Method: 通过在学习代码书索引的分类后验的同时，在连续嵌入空间中计算速度场，将变分流匹配适应于向量量化潜在空间。

Result: 在ImageNet-1k 256x256生成任务上，训练收敛速度比连续流匹配和离散流匹配基线更快，同时达到与最先进模型竞争性的FID分数。

Conclusion: 变分流匹配可以有效桥接连续传输和离散监督，提高图像生成的训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出了一种统一的深度学习框架，通过条件扩散模型和多任务学习，从非对比CT扫描生成合成对比增强CT图像，同时分割主动脉腔和血栓，避免了多阶段方法的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 传统对比增强CT需要碘对比剂，存在肾毒性、过敏反应和环境危害等风险。现有方法采用多阶段流程，导致误差累积且无法充分利用共享的语义和解剖结构。

Method: 集成条件扩散模型与多任务学习，实现端到端的图像合成和解剖分割联合优化。无需初始预测，共享编码器和解码器参数，采用半监督训练处理缺失分割标签的临床数据。

Result: 在264名患者队列中表现优于现有方法：图像合成PSNR达25.61 dB，腔分割Dice得分0.89，血栓分割Dice得分0.53，腔直径MAE降至4.19 mm，血栓面积误差降至33.85%。

Conclusion: 该统一框架在减少对比剂使用的同时，提高了图像质量和分割精度，为临床主动脉瘤评估提供了更安全准确的解决方案。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 提出了一个用于多模态内容分析的高效原型框架，将预训练模型与视频数据结合，生成可查询的知识图谱表示


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析复杂、计算成本高且需要大量工程努力，现有预训练模型与视频等复杂数据的融合具有挑战性

Method: 设计候选流程配方，结合预训练模型将视频转换为时序半结构化数据格式，再转换为帧级索引知识图谱表示

Result: 创建了可查询的知识图谱，支持持续学习，能够通过交互方式动态整合新的领域特定知识

Conclusion: 该框架能够高效原型化多模态内容分析流程，解决了视频与预训练模型融合的挑战

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT框架通过逆向工程将网站功能转化为可重用的工具，让Web代理直接调用高级操作（如搜索、筛选、排序），而不是依赖脆弱的逐步UI交互，从而在浏览器自动化中实现更高的成功率和更少的步骤。


<details>
  <summary>Details</summary>
Motivation: 当前Web代理方法依赖逐步UI交互和大量LLM推理，在动态布局和长任务中表现脆弱。相比之下，人类通过网站提供的高级功能（如搜索、筛选、排序）来完成任务。

Method: WALT框架逆向工程网站的潜在功能，将其转化为可调用的工具，涵盖发现（搜索、筛选、排序）、通信（发布、评论、点赞）和内容管理（创建、编辑、删除）等功能。

Result: 在VisualWebArena和WebArena测试中，WALT实现了更高的成功率、更少的步骤和更少的LLM依赖推理。

Conclusion: WALT建立了一个稳健且可推广的浏览器自动化范式，将计算负担从脆弱的逐步推理转移到可靠的工具调用上。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 提出了一种用于组织病理学图像半监督分割的框架，通过拓扑一致性机制来识别和保留有意义的语义结构，减少拓扑错误。


<details>
  <summary>Details</summary>
Motivation: 在半监督分割中，从未标记数据中捕获有意义的语义结构至关重要，特别是在组织病理学图像分析中，物体密集分布，这一挑战尤为突出。

Method: 利用通过随机dropout和时间训练快照获得的多个扰动预测，在这些不同输出之间强制实施拓扑一致性，并引入结合空间重叠和全局结构对齐的新型匹配策略。

Result: 广泛实验表明，该方法有效减少了拓扑错误，产生了更稳健和准确的分割结果。

Conclusion: 该方法对于可靠的下游分析至关重要，能够生成更稳健和准确的分割结果。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion-LPO框架，用于在扩散模型中实现列表偏好优化，通过利用包含排名信息的用户反馈来更精确地对齐人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法主要依赖成对偏好，但人类对图像的偏好反馈通常包含隐含的排名信息，这比成对比较能传达更精确的人类偏好。

Method: 基于Plackett-Luce模型推导了DPO目标的列表扩展，给定标题时将用户反馈聚合成图像排名列表，通过鼓励每个样本优于其所有低排名替代品来在整个排名中保持一致性。

Result: 在文本到图像生成、图像编辑和个性化偏好对齐等任务中，Diffusion-LPO在视觉质量和偏好对齐方面始终优于成对DPO基线方法。

Conclusion: Diffusion-LPO是一个简单有效的框架，能够更好地利用列表偏好数据来优化扩散模型与人类偏好的一致性。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge是一个纯自回归的统一多模态大语言模型，通过混合Transformer架构和语义到像素的离散表示，在单一的下一个token预测框架中同时支持图像理解和生成。


<details>
  <summary>Details</summary>
Motivation: 构建统一的多模态大语言模型面临挑战：混合方法破坏了自回归范式，而纯自回归方法在语义对齐和像素级保真度之间存在权衡。

Method: 采用混合Transformer架构，通过语义到像素的离散表示整合紧凑语义token和细粒度像素token，在保持语言对齐的同时精确描述视觉细节。

Result: 在多个多模态基准测试中，Bridge在理解和生成任务上都取得了竞争性或更优的结果，同时需要更少的训练数据和训练时间。

Conclusion: Bridge证明了在单一自回归框架内实现高质量图像理解和生成的可行性，为统一多模态模型提供了有效解决方案。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 提出了一种结合CNN和贝叶斯深度学习的混合模型，用于口腔癌分类，特别适用于小训练数据集。该方法通过变分推理进行不确定性量化，提高了模型的可靠性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 口腔癌是全球高发癌症，早期诊断对降低死亡率至关重要。传统深度学习模型需要大量数据，在医疗资源匮乏地区难以实现，且存在过度自信和可靠性不足的问题。

Method: 结合卷积神经网络(CNN)和贝叶斯深度学习，使用变分推理进行不确定性量化，在小型训练集上训练模型。

Result: 在训练数据分布相似的测试集上达到94%准确率，与传统CNN相当；在真实世界照片数据上表现更优，达到88%准确率（传统CNN为72.94%）。模型对正确分类样本显示低不确定性，对错误分类样本显示高不确定性。

Conclusion: 贝叶斯推理在数据稀缺环境下能有效提升口腔癌早期诊断的模型可靠性和泛化能力。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 提出CADTrans方法，通过构建辅助域和一致性策略来解决无源域自适应中的不变特征表示问题，并使用条件多核最大均值差异来对齐困难样本。


<details>
  <summary>Details</summary>
Motivation: 由于无法直接访问源域数据，现有方法难以获得确定性的不变特征，且容易受到困难样本和域偏差的影响。

Method: 开发辅助域模块获取多样化表示，通过一致性策略获得不变特征表示，并使用CMK-MMD策略对齐困难样本和简单样本。

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等基准测试上取得了显著的性能提升。

Conclusion: CADTrans方法有效解决了无源域自适应中的不变特征表示问题，并显著提升了性能。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 开发了一个系统，利用BLV用户的历史视觉问题来指导多模态大语言模型生成更相关的图像描述，而非默认的冗长描述。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM应用通常提供全面的冗长描述，缺乏上下文相关性，导致BLV用户需要筛选大量无关信息来获取所需内容。

Method: 系统基于VizWiz-LF数据集，识别相似的历史视觉上下文，利用相关的问题来指导MLLM生成更符合BLV用户需求的描述。

Result: 评估显示，76.1%的情境感知描述能够预测并回答用户问题，在54.4%的比较中被优先选择。

Conclusion: 利用BLV用户的历史视觉问题可以有效指导MLLM生成更相关、更高效的图像描述，提升用户体验。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 开发了ImageNet-Think数据集，包含25万张ImageNet21k图像，提供结构化思维标记和答案，用于训练具有显式推理能力的视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 促进具有显式推理能力的视觉语言模型发展，帮助理解多模态推理机制。

Method: 使用两个最先进的视觉语言模型（GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506）生成合成数据集，每张图像配有两对思维-答案序列。

Result: 创建了包含逐步推理过程和最终描述性答案的多模态推理数据集。

Conclusion: 该数据集将公开可用，以支持推理/思维多模态视觉语言模型的研究。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出了一种新的正则化方法NPN，通过神经网络学习感知矩阵零空间的低维投影，而不是在图像域施加结构约束，从而提升图像逆问题重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法忽略了感知矩阵零空间的任务特定结构，导致重建质量受限。作者希望利用零空间的结构信息来设计更有效的先验。

Method: 提出非线性的零空间投影(NPN)正则化，使用神经网络学习感知矩阵零空间的低维投影，并将其集成到现有重建框架中。

Result: 在压缩感知、去模糊、超分辨率、CT和MRI等多种成像逆问题中，NPN先验能一致地提升重建保真度，与多种重建方法兼容。

Conclusion: NPN方法通过关注零空间结构，提供了可解释且灵活的正则化策略，能有效提升各种成像逆问题的重建性能。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出自动化基因组解释模块，将原始DNA序列转化为可解释的决策，结合混沌游戏表示和概念瓶颈模型，通过生物学概念进行预测，并集成可靠性增强技术。


<details>
  <summary>Details</summary>
Motivation: 解决基因组数据解释与自动化决策之间的差距，为医疗自动化和机器人系统提供可靠、可解释的基因组分析基础。

Method: 结合混沌游戏表示(CGR)和概念瓶颈模型(CBM)，通过GC含量、CpG密度、k-mer基序等生物学概念进行预测，采用概念保真度监督、先验一致性对齐、KL分布匹配和不确定性校准等技术增强可靠性。

Result: 在HIV亚型分类任务中达到最先进性能，在内部和LANL数据集上均表现优异，具有更好的概念预测保真度和成本效益权衡。

Conclusion: 该工作为基因组医学中的机器人和临床自动化建立了可靠基础，桥接了可解释基因组建模与自动化决策之间的鸿沟。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1是一个增强推理能力的视觉-语言-动作模型，通过强化学习从可验证奖励和组相对策略优化来系统优化推理和执行，在泛化性和真实世界性能上优于现有VLA方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型缺乏显式的逐步推理，直接输出最终动作而不考虑可供性约束或几何关系，且后训练流程很少强化推理质量，主要依赖监督微调和弱奖励设计。

Method: 提出VLA-R1模型，集成RLVR（从可验证奖励的强化学习）和GRPO（组相对策略优化），设计基于RLVR的后训练策略，包含区域对齐、轨迹一致性和输出格式化的可验证奖励，并构建VLA-CoT-13K高质量数据集提供与可供性和轨迹标注明确对齐的思维链监督。

Result: 在领域内、领域外、仿真和真实机器人平台上的广泛评估表明，VLA-R1相比先前VLA方法实现了更优越的泛化性和真实世界性能。

Conclusion: VLA-R1通过系统优化推理和执行，显著提升了VLA模型的推理鲁棒性和执行准确性，为具身AI提供了更强大的解决方案。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种联合去模糊和3D重建的方法，用于解决微距摄影中的散焦模糊问题，通过可微分渲染自监督优化3D模型和散焦模糊核。


<details>
  <summary>Details</summary>
Motivation: 微距摄影具有高分辨率和大放大倍率的优势，但散焦模糊严重阻碍了清晰成像和高质量3D重建。传统方法需要大量图像和标注，且目前没有针对微距摄影的多视角3D重建方法。

Method: 从多视角模糊图像出发，联合优化物体的清晰3D模型和每个像素的散焦模糊核。整个框架采用可微分渲染方法来自监督优化3D模型和散焦模糊核。

Result: 大量实验表明，从少量多视角图像出发，该方法不仅能实现高质量图像去模糊，还能恢复高保真度的3D外观。

Conclusion: 该方法成功解决了微距摄影中的散焦模糊问题，实现了联合去模糊和3D重建，为小细节物体的高质量3D建模提供了有效解决方案。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一种新颖的单步扩散模型，通过将运动去模糊重新表述为扩散过程，训练一致性模型实现高保真度的一步去模糊，在保持高质量的同时显著减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的去模糊方法存在推理时间过长和保真度不足的问题，限制了其在真实世界应用中的潜力。

Method: 将运动去模糊重新表述为扩散过程，每个时间步对应逐渐模糊的图像；训练一致性模型使所有时间步对齐到同一清晰图像；集成Kernel ControlNet进行模糊核估计；引入自适应时间步预测。

Result: 在全参考指标上实现卓越性能，超越先前的扩散方法，与其他最先进模型性能相当。

Conclusion: FideDiff为预训练扩散模型在高保真图像恢复任务中的应用提供了新方向，为工业应用建立了强大基准。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 本文提出了一个用于青铜器铭文识别的两阶段检测-识别流程，通过LadderMoE架构处理多域变异性，在大型数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 青铜器铭文识别面临视觉退化严重、多域变异性（照片、拓片、摹本）和长尾字符分布等挑战，需要开发专门的识别方法。

Method: 构建大规模BI数据集，采用两阶段检测-识别流程，使用LadderMoE增强预训练CLIP编码器，实现动态专家专门化和更强鲁棒性。

Result: 在单字符和全页识别任务上显著优于最先进的场景文本识别基线，在头部、中部和尾部类别以及所有采集模态上都取得优越准确率。

Conclusion: 为青铜器铭文识别和下游考古分析建立了坚实基础。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA提出了一种通过视觉重编程进行领域自适应的方法，仅需在骨干网络前添加领域特定的视觉重编程层，无需修改骨干网络参数，实现参数高效和模型复用。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法需要为每个新的源-目标对微调骨干网络参数，导致训练参数和存储内存线性增长，且无法复用训练好的骨干网络参数。

Method: 在骨干网络前添加领域特定的视觉重编程层，生成视觉提示作为纹理偏置，通过优化域内和域间分布差异的目标函数来训练这些层，而不修改骨干网络参数。

Result: 在Office-31数据集上达到92.8%的平均准确率，仅需1.5M可训练参数，优于现有参数高效的UDA基线方法，且参数使用量显著减少。

Conclusion: VirDA通过视觉重编程实现了参数高效的领域自适应，在保持高性能的同时大幅减少了训练参数，支持骨干网络的跨域复用。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 提出了一种名为离散面部编码(DFE)的无监督方法，通过残差向量量化变分自编码器从3D网格序列中学习紧凑且可解释的面部表情字典，在心理学任务中优于FACS和其他面部编码方法。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情编码系统如FACS存在覆盖范围有限和手动标注成本高的问题，需要一种数据驱动的替代方案来更精确地捕捉面部行为。

Method: 使用3D形变模型提取身份不变的表情特征，然后通过残差向量量化变分自编码器将这些特征编码为来自共享码本的离散标记序列，每个标记捕获特定的可重用面部变形模式。

Result: DFE比FACS和其他面部编码方法捕捉到更精确的面部行为，在压力检测、性格预测和抑郁检测等心理学任务中表现优于FACS流程和强图像/视频表示学习模型。

Conclusion: DFE作为一种可扩展且有效的FACS替代方案，在心理学和情感计算应用中具有潜力，能够覆盖更广泛的面部表情。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: 提出了一种名为Con-NRSfM的新方法，用于处理共形变形下的非刚性结构恢复问题，该方法通过图优化框架进行点对点重建，能够准确计算局部共形尺度并实现更精确的深度估计。


<details>
  <summary>Details</summary>
Motivation: 现有非刚性结构恢复方法依赖严格假设（如局部平面表面或局部线性变形），无法恢复共形尺度，限制了在单目视觉可变形SLAM中的应用。

Method: 使用2D选择的图像扭曲进行点对点重建，通过图优化框架优化；采用并行可分离迭代优化策略解决公式化问题的敏感性；结合自监督学习框架生成带纹理的密集3D点云。

Result: 在合成和真实数据集上的仿真和实验结果表明，该方法在重建精度和鲁棒性方面优于现有方法。

Conclusion: Con-NRSfM方法成功消除了现有方法的限制，能够准确计算局部共形尺度，实现更精确的深度估计，为非刚性结构恢复提供了更有效的解决方案。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse提出了一种将鲁棒3D重建分解为修复和重建两个子任务的统一框架，使用视频扩散模型处理不一致的多视角图像，实现更好的3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 解决从不一致多视角图像进行鲁棒3D重建的挑战，现有方法依赖密集观测且优化困难，需要一种能处理各种图像不一致性的通用方法。

Method: 将鲁棒重建分解为修复和重建两个子任务：首先将不一致图像转换为初始视频，然后使用专门设计的视频扩散模型修复为一致图像，最后从修复后的图像重建3D场景。

Result: 在合成和真实数据集上的广泛实验表明，该方法具有强大的泛化能力和优越的鲁棒重建性能，并能控制重建3D场景的风格。

Conclusion: UniVerse通过分解任务和使用扩散模型学习通用场景先验，有效解决了从不一致多视角图像进行3D重建的问题，相比逐视图退化建模方法具有更好的泛化性。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级端到端模板匹配框架，将模板匹配重新定义为联合定位和几何回归问题，输出中心坐标、旋转角度和独立缩放比例，解决了传统方法效率低和深度学习方法无法显式建模几何姿态的问题。


<details>
  <summary>Details</summary>
Motivation: 工业检测和组件对准任务中，传统模板匹配方法在复合变换下效率低下，而大多数基于深度学习的方法只估计相似度分数而无法显式建模几何姿态，难以在实际部署中使用。

Method: 使用模板感知动态卷积模块(TDCM)在推理时动态注入模板特征，采用深度可分离卷积和像素重排实现高效匹配，通过旋转-剪切增强策略和结构感知伪标签实现无几何标注训练，并添加轻量级细化模块提升角度和尺度精度。

Result: 3.07M参数的模型在复合变换下实现高精度和14ms推理速度，在小模板和多目标场景中表现出强鲁棒性，适合实时工业应用部署。

Conclusion: 该框架通过联合定位和几何回归方法，结合动态模板特征注入和高效网络设计，实现了高精度、高效率的模板匹配，为实时工业应用提供了可行的解决方案。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了首个自适应像素推理框架，通过动态决定何时需要像素级操作来解决视觉语言模型在细粒度视觉任务中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理需要精确理解细粒度视觉元素的任务时表现不佳，主要由于图像编码过程中的信息丢失或对关键区域关注不足。现有方法引入像素级信息但存在过度使用问题，导致效率低下和无关视觉细节干扰。

Method: 首先通过操作感知的监督微调建立文本推理和视觉操作的基础能力，然后设计新颖的rollout引导的强化学习框架，基于模型自身响应的反馈来动态决定何时调用像素操作。

Result: 在广泛的多模态推理基准测试中，模型实现了优越性能同时显著减少不必要的视觉操作。在HR-Bench 4K上达到73.4%准确率，工具使用率仅为20.1%，相比之前方法在提高准确率的同时减少66.5%的工具使用。

Conclusion: 提出的自适应像素推理框架有效解决了视觉语言模型在细粒度视觉任务中的效率和精度平衡问题，通过动态决策机制实现了高性能和低资源消耗的统一。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出了一种基于增强敏感性的风险评分框架（ASRS），用于识别胸片分析中容易出错的病例，通过测量图像旋转后的嵌入变化来评估模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胸片解读中存在公平性和可靠性问题，模型在不同患者亚组中表现不均，现有错误检测方法难以处理分布内的细微错误。

Method: 使用临床合理的旋转（±15°/±30°）增强图像，通过RAD-DINO编码器测量嵌入变化，基于敏感性评分将样本分为稳定性四分位数。

Result: 高敏感性病例的召回率显著降低（-0.2到-0.3），尽管具有高AUROC和置信度，ASRS能够有效识别错误倾向病例。

Conclusion: ASRS提供了一种无需标签的选择性预测和临床医生审查方法，提高了医疗AI的公平性和安全性。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一个无需训练的视频风格化框架，通过整合多个风格化参考到预训练的图转视频模型，生成具有丰富风格细节和强时间一致性的风格化视频。


<details>
  <summary>Details</summary>
Motivation: 现有的视频风格化方法存在时间一致性差、风格丰富度不足的问题，而训练专门的视频风格化模型需要配对视频数据且计算成本高昂。

Method: 整合多个风格化参考到预训练图转视频模型，使用高频补偿约束内容布局和运动，结合基于光流的运动线索在低显著性区域保持风格纹理。

Result: FreeViS在风格化保真度和时间一致性方面表现优异，超越了现有基线方法，获得了强烈的人类偏好。

Conclusion: 该无需训练的流程为高质量、时间一致性的视频风格化提供了实用且经济的解决方案。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: 提出了MedQ-Bench基准测试，通过感知-推理范式评估多模态大语言模型在医学图像质量评估中的能力，包含感知和推理两个任务，涵盖5种成像模态和40多个质量属性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估方法受限于标量评分指标，无法反映专家评估中的人类推理过程，需要建立更符合人类推理的语言评估范式。

Method: 设计MedQ-Bench基准，包含MedQ-Perception（感知任务）和MedQ-Reasoning（推理任务），涵盖5种成像模态、40多个质量属性，共2600个感知查询和708个推理评估。

Result: 评估14个最先进MLLMs显示模型具有初步但不稳定的感知和推理能力，准确率不足以可靠临床使用，需要针对性优化。

Conclusion: MedQ-Bench将推动MLLMs在医学图像质量评估领域的进一步探索和潜力开发。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer是一个能够通过单次前向传播从RGB图像中预测完整遮挡和深度顺序的网络，无需昂贵的输入格式和推理成本。


<details>
  <summary>Details</summary>
Motivation: 理解实例级几何关系在视觉模型中具有挑战性，现有方法需要昂贵的输入格式（类别标签、分割掩码）和二次方的前向传播推理成本。

Method: InstaFormer通过对象查询与潜在掩码描述符之间的交互来实现整体顺序预测，这些描述符语义上表示相同对象但携带互补信息。

Result: 通过全面的基准测试和消融实验验证了该方法的有效性。

Conclusion: InstaFormer能够以单次前向传播从RGB图像中预测场景中所有实例的完整遮挡和深度顺序，代码和模型已开源。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，通过金字塔位置编码和强化学习优化，实现了高效的高分辨率图像风格化处理。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率输入时效率低下，需要更高效的神经风格迁移方法。

Method: 提出PyramidStyler框架，采用金字塔位置编码(PPE)来捕获局部细节和全局上下文，并引入强化学习动态优化风格化过程。

Result: 在4000轮训练后，内容损失降低62.6%至2.07，风格损失降低57.4%至0.86，推理时间1.39秒；使用RL后进一步改善为内容损失2.03，风格损失0.75，速度仅轻微下降至1.40秒。

Conclusion: 该方法实现了实时高质量的艺术渲染，在媒体和设计领域具有广泛应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一个负载均衡的高效3D高斯泼溅框架，通过深度感知分区、优化策略和轻量级技术，解决了大规模场景中3DGS的扩展性问题，实现了2倍训练速度提升。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯泼溅在大规模无边界场景（如城市街区）中的扩展性难题，现有分治方法存在负载不均衡和粗到细流水线效率低下的问题。

Method: 引入深度感知分区方法（将预处理从小时级降至分钟级）、基于优化的可见高斯分布均衡策略、可见性裁剪和选择性致密化两种轻量级技术。

Result: 在大规模城市和户外数据集上，LoBE-GS相比最先进基线实现了2倍端到端训练速度提升，同时保持重建质量，能够扩展到原始3DGS无法处理的场景。

Conclusion: LoBE-GS通过重新设计大规模3DGS流水线，有效解决了负载均衡和效率问题，为大规模场景的实时高保真3D重建提供了可行方案。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing两个创新方法，解决长视频生成中的长期依赖建模和自回归解码误差累积问题，提升分钟级时间一致性和推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕获长期依赖关系，同时防止自回归解码中固有的误差累积。

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐，减少推理过程中的误差传播。

Result: 实现了分钟级时间一致性，计算效率高且保持线性复杂度，显著提升了长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing共同增强了自回归视频模型的实用性和长视频生成的可靠性。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 该论文提出了两种辅助正则化损失项来改善3D目标检测器的分类置信度校准，重点关注完整预测分布的校准，并在多个检测器上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶系统中，精确的目标检测和不确定性估计对于系统的自我感知和安全运行至关重要，需要解决3D目标检测器分类任务的置信度校准问题。

Method: 提出了两种辅助正则化损失项：一种校准主导预测，另一种校准完整预测向量；结合等渗回归方法在CenterPoint、PillarNet和DSVT-Pillar等检测器上进行评估。

Result: 发现结合完整类别预测校准损失项和等渗回归的方法在CenterPoint和PillarNet上对主导和次要类别预测都能实现最佳校准效果，但DSVT-Pillar无法使用相同方法同时校准主导和次要预测。

Conclusion: 完整预测分布的校准对于3D目标检测器的置信度校准至关重要，提出的方法在多个检测器上表现出良好效果，但不同检测器可能需要不同的校准策略。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS是一个利用预训练扩散模型进行行人搜索的新框架，通过三个专门模块解决现有方法中检测和重识别任务间的优化冲突问题，在两个基准数据集上达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有行人搜索方法主要使用ImageNet预训练骨干网络，可能无法有效捕捉复杂空间上下文和细粒度身份线索，且共享骨干网络特征会导致检测和重识别任务间的优化冲突。

Method: 提出DiffPS框架，利用预训练扩散模型，包含三个专门模块：扩散引导区域提议网络(DGRPN)用于改进行人定位，多尺度频率细化网络(MSFRN)缓解形状偏差，语义自适应特征聚合网络(SFAN)利用文本对齐的扩散特征。

Result: 在CUHK-SYSU和PRW数据集上达到了新的最先进水平。

Conclusion: 扩散先验知识能够有效提升行人搜索性能，通过专门设计的模块可以解决检测和重识别任务间的优化冲突问题。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出Flow-Matching-guided Unfolding网络(FMU)，首次将流匹配集成到高光谱图像重建中，通过嵌入生成先验到深度展开框架，并引入平均速度损失来增强流一致性。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像成本高，压缩感知系统如CASSI虽然提高了效率，但重建仍面临严重退化问题，难以恢复精细光谱细节。

Method: 将流匹配的生成先验嵌入深度展开框架，引入平均速度损失来增强流动力学的一致性，结合基于优化方法的可解释性和流匹配的生成能力。

Result: 在模拟和真实数据集上的大量实验表明，FMU在重建质量上显著优于现有方法。

Conclusion: FMU通过流匹配引导的展开网络成功提升了高光谱图像重建质量，代码和模型将开源。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 提出基于深度学习的双列直插封装自动缺陷检测系统，使用ConSinGAN生成训练数据，YOLOv7结合ConSinGAN在准确率和检测时间上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统工业组件缺陷检测耗时耗力，给质检人员带来负担且难以管理产品质量。

Method: 使用数字相机光学和深度学习模型，针对DIP的表面缺陷和引脚缺陷，采用ConSinGAN生成数据集，比较YOLOv3/v4/v7/v9模型性能。

Result: YOLOv7结合ConSinGAN达到95.50%准确率，检测时间285毫秒，远优于基于阈值的方法。

Conclusion: 所提出的自动缺陷检测系统可轻松应用于多种缺陷类型或缺陷数据不足的情况。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: FoundAD是一个基于基础视觉编码器的少样本异常检测方法，通过学习非线性投影算子到自然图像流形，有效识别图像中的异常区域，在参数更少的情况下实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 少样本异常检测在工业安全检查中很重要，但有限样本使得正常与异常特征难以区分，特别是在类别无关条件下。基础视觉编码器的大规模预训练有助于学习正常图像的通用分布。

Method: 利用图像中异常量与学习嵌入差异的相关性，设计非线性投影算子到自然图像流形，该算子作为异常检测工具来表征和识别图像中的分布外区域。

Result: 广泛实验表明该方法支持多类检测，在使用显著更少参数的情况下达到竞争性性能，并通过多个基础编码器（包括DINOv3）的评估验证了有效性。

Conclusion: 该方法为基础特征的应用提供了新视角，并推动了少样本异常检测领域的发展。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT通过可训练的聚类模块和再生器模块优化Vision Transformer，在语义分割任务中显著降低计算复杂度并保持准确率


<details>
  <summary>Details</summary>
Motivation: Vision Transformer在真实机器人系统中应用受限，因为其二次注意力复杂度。现有token合并方法适合分类但不适合密集预测任务

Method: 在ViT骨干网络中增加可训练的聚类模块，根据分割掩码的伪聚类指导合并相似token，然后通过再生器模块恢复细节信息

Result: 在三个数据集上实现GFLOPs减少2.18倍，推理速度提升1.64倍，同时保持可比较的分割准确率

Conclusion: ClustViT有效解决了ViT在密集预测任务中的计算效率问题，为实际机器人应用提供了可行方案

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: 提出了PaDT（Patch-as-Decodable Token）统一范式，使多模态大语言模型能够直接生成文本和多样化视觉输出，解决了现有方法依赖间接表示的限制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视觉任务中依赖间接表示（如将坐标生成为文本），这限制了性能并阻碍了密集预测任务如分割。

Method: 引入视觉参考标记（VRTs），从查询图像的视觉补丁嵌入中提取，并与LLM的输出文本标记无缝交织。使用轻量级解码器将LLM输出转换为检测、分割和定位预测。

Result: 在四个视觉感知和理解任务上的实证研究表明，PaDT始终达到最先进的性能，甚至与显著更大的MLLM模型相比。

Conclusion: PaDT通过统一范式有效解决了多模态大语言模型在视觉任务中的限制，实现了直接生成多样化视觉输出的能力。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 该论文针对生鲜电商中的信任赤字问题，提出了TriAlignXA可解释AI框架，通过多目标优化解决农产品分级中的'不可能三角'（生物特性、时效性、经济可行性），并设计了三角信任指数(TTI)来量化权衡。


<details>
  <summary>Details</summary>
Motivation: 在线生鲜电商存在信任赤字，因为数字交易无法提供对产品质量的直接感官感知。传统绝对分级标准在农产品分级中存在局限性，无法同时满足生物特性、时效性和经济可行性的要求。

Method: 构建'信任金字塔'模型，提出TriAlignXA可解释AI框架，包含三个核心引擎：生物自适应引擎（精细质量描述）、时效优化引擎（处理效率）、经济优化引擎（成本控制）。采用'预映射机制'将过程数据编码为二维码，透明传递质量信息。

Result: 在分级任务实验中，准确率显著高于基线模型。实证证据和理论分析验证了框架在解决'不可能三角'方面的平衡能力。

Conclusion: 该研究为构建可信赖的在线农产品生态系统提供了从理论到实践的全面支持，建立了从算法决策到消费者信任的关键路径。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft是一个一致的交互式4D高斯溅射编辑框架，通过4D感知的InstructPix2Pix模型、多视图网格模块和基于LLM的用户意图理解模块，解决了视图、时间和非编辑区域一致性问题，并能处理复杂文本指令。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯溅射编辑方法在视图一致性、时间一致性、非编辑区域一致性以及处理复杂文本指令方面仍面临挑战，需要开发更一致和可控的4D场景编辑框架。

Method: 1. 引入4D感知InstructPix2Pix模型，结合4D VGGT几何特征；2. 使用多视图网格模块迭代优化多视图输入图像；3. 提出高斯选择机制保护非编辑区域；4. 设计基于LLM的用户意图理解模块，将复杂指令分解为原子操作序列。

Result: 相比相关工作，该方法实现了更一致和可控的4D场景编辑，能够有效处理复杂的用户指令并保持编辑质量。

Conclusion: 4DGS-Craft框架通过结合几何感知、多视图优化和智能指令解析，显著提升了4D高斯溅射编辑的一致性和交互性，为复杂4D场景编辑提供了有效解决方案。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出Pure-Pass (PP)像素级掩码机制，通过固定颜色中心点分类像素，免除纯像素的昂贵计算，集成到ATD-light模型中实现高效超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法如CAMixer存在适应性差、掩码粒度粗、空间灵活性不足等问题，需要更精细的计算优化。

Method: 使用Pure-Pass像素级掩码机制，基于固定颜色中心点对像素分类，识别纯像素并免除其计算，集成到ATD-light模型中。

Result: PP-ATD-light在节省相似计算量的情况下，在重建质量和参数效率上均优于CAMixer-ATD-light。

Conclusion: Pure-Pass机制实现了细粒度、空间灵活的自适应掩码，以最小开销提升超分辨率性能。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 使用GPT-4o和自校正循环框架自动生成颌骨囊肿的放射学发现，通过结构化输出和一致性检查提高了牙齿编号等项目的准确性。


<details>
  <summary>Details</summary>
Motivation: 利用多模态AI自动生成颌骨囊肿的放射学发现，提高诊断效率和准确性。

Method: 构建自校正循环结构化输出(SLSO)框架，包括图像分析、结构化数据生成、牙齿编号提取和一致性检查、迭代再生等10步流程。

Result: 与CoT方法相比，SLSO在牙齿编号、牙齿移动和牙根吸收方面的准确率分别提高了66.9%、33.3%和28.6%。成功案例经过最多5次再生后获得一致的结构化输出。

Conclusion: SLSO框架能强制阴性发现描述、抑制幻觉、提高牙齿编号识别准确性，但对跨多颗牙齿的大范围病变识别仍有限制，需要进一步改进以实现实用的发现生成系统。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: 提出LiLa-Net 3D自编码器架构，仅使用LiDAR点云从真实交通环境中提取高效特征，通过简化编码器层和跳跃连接实现准确重建。


<details>
  <summary>Details</summary>
Motivation: 开发一种仅依赖LiDAR点云的高效3D自编码器，用于真实交通环境特征提取，在保持性能的同时减少资源消耗。

Method: 采用简化的编码器层和跳跃连接架构，平衡跳跃连接信息和潜在编码，构建高效的潜在空间表示。

Result: 模型能够准确重建原始点云，在重建质量与性能之间取得良好平衡，并展现出对非交通环境对象的强泛化能力。

Conclusion: LiLa-Net通过简化架构实现了高效的点云重建，在资源消耗和性能之间取得平衡，具有良好的泛化能力。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: 开发了kabr-tools开源工具包，通过无人机视频和机器学习自动监测多物种行为，显著提高了行为观测的粒度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统野外观察方法范围有限、耗时费力，难以评估跨景观的行为响应，需要可扩展的方法来量化复杂多维行为模式。

Method: 整合无人机视频与机器学习系统，利用目标检测、跟踪和行为分类技术，提取行为、社交和空间指标。

Result: 相比地面方法，无人机观测将可见度损失减少15%，捕获更多行为转换，准确性和连续性更高。验证了969个行为序列，发现不同物种的行为差异和空间隔离。

Conclusion: kabr-tools实现了规模化自动行为监测，为生态系统研究、保护和生态监测提供了强大工具。

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一个从多视角图像进行语义感知3D形状和纹理变形的新框架，通过网格引导的3D高斯溅射实现高保真几何和外观建模，无需标记数据即可保持局部细节和全局语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖点云或需要预定义同胚映射来处理无纹理数据，存在局限性。本文旨在克服这些限制，实现语义感知的3D形状和纹理变形。

Method: 使用网格引导的3D高斯溅射(3DGS)进行高保真建模，核心是统一变形策略，将3D高斯锚定到重建的网格面片上，通过拓扑感知约束确保几何一致性和纹理保真度，同时利用网格拓扑作为几何先验建立无监督语义对应。

Result: 在提出的TexMorph基准测试中，GaussianMorphing显著优于先前的2D/3D方法，颜色一致性误差(ΔE)降低22.2%，EI降低26.2%。

Conclusion: 该框架能够在不需要标记数据的情况下，在整个变形过程中保持局部细节和全局语义一致性，为3D形状和纹理变形提供了有效的解决方案。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出InPose方法，使用预训练扩散模型仅基于旋转测量进行姿态估计，通过似然项引导生成符合位置测量的姿态序列，实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散模型的方法泛化能力差，主要因为位置测量受用户体型影响大，需要解决跨用户泛化问题。

Method: 将姿态估计建模为逆问题，使用预训练扩散模型仅以旋转测量为条件，通过从位置测量推导的似然项来引导生成姿态序列。

Result: 提出的InPose方法能够为零样本泛化生成高度可能的姿态序列，准确解释稀疏的体上测量数据。

Conclusion: 通过将姿态估计作为逆问题处理，并分离旋转和位置测量的使用，实现了跨用户的零样本泛化能力。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出VGDM框架，结合视觉Transformer和扩散模型进行脑肿瘤检测与分割，通过全局上下文推理和迭代去噪提升体积精度和边界精度。


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构如U-Net在捕捉长距离依赖关系方面有限，限制了在复杂肿瘤结构上的性能。扩散模型在生成高保真医学图像和细化分割边界方面展现出强大潜力。

Method: 在扩散过程核心嵌入视觉Transformer，利用Transformer骨干网络建模整个MRI体积的空间关系，同时通过扩散细化减轻体素级误差并恢复细粒度肿瘤细节。

Result: 在MRI脑肿瘤数据集上的实验验证显示，在Dice相似度和Hausdorff距离指标上获得一致提升。

Conclusion: Transformer引导的扩散模型有潜力推动肿瘤分割技术发展，超越传统U-Net基线，为神经肿瘤学提供改进的鲁棒性和可扩展性路径。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 开发可扩展的深度学习管道，从1925-1950年历史地图中提取法国全国城市足迹，填补1970年前城市扩张定量分析的数据空白


<details>
  <summary>Details</summary>
Motivation: 1970年代前法国历史城市扩张定量分析因缺乏全国数字城市足迹数据而受阻，需要填补这一数据空白

Method: 采用双通道U-Net方法处理历史地图的高辐射度和风格复杂性，第一通道生成初步地图识别混淆区域，第二通道使用精炼数据集和第一模型二值化输出最小化辐射噪声

Result: 处理了覆盖法国本土的941个高分辨率图块，最终镶嵌图总体准确率达到73%，有效捕捉多样化城市模式并克服标签和等高线等常见伪影

Conclusion: 成功创建首个开放获取的全国尺度城市足迹数据集，为长期城市化动态研究提供支持，并公开代码、训练数据集和结果

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 系统分析腹腔镜胆囊切除术视频中点追踪的失败模式，发现点追踪在手术工具上表现良好，但在解剖目标上表现不佳，主要因组织相似性和模糊边界导致失败。


<details>
  <summary>Details</summary>
Motivation: SAM2等视频对象分割模型在手术视频中具有零样本追踪能力，但点追踪在复杂手术环境中的可靠性和失败情况尚不明确，需要系统分析。

Method: 在腹腔镜胆囊切除术视频中，针对胆囊、抓钳和L型电钩三个手术目标，比较点追踪与分割掩码初始化的性能表现。

Result: 点追踪在手术工具上表现竞争性，但在解剖目标上持续表现不佳，主要由于组织相似性和模糊边界导致追踪失败。

Conclusion: 通过定性分析揭示了影响追踪结果的关键因素，并提供了选择与放置追踪点的实用建议，以改进手术视频分析性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 提出了FFREEDG任务：在服务器上预训练模型后，仅使用客户端的未标记数据进行联邦学习，不再访问源数据。提出FRIEREN框架，利用视觉基础模型和视觉-语言解码器解决该任务。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在语义分割任务中面临域偏移挑战，特别是当客户端数据未标记时。现有方法要么不切实际地假设能访问标记数据，要么未能充分利用现代视觉基础模型的能力。

Method: 使用视觉-语言解码器，基于CLIP文本嵌入改进语义消歧，采用弱到强一致性学习策略在伪标签上进行鲁棒的本地训练。

Result: 在合成到真实和清晰到恶劣天气基准测试中，框架有效解决了新任务，性能与现有域泛化和适应方法相当，为未来研究建立了强基线。

Conclusion: FRIEREN框架成功解决了FFREEDG这一具有挑战性的任务，展示了利用视觉基础模型和视觉-语言融合在联邦学习中的有效性。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint是一个基于动作知识的结构化提示框架，通过细粒度指导问题提升冻结视觉语言模型在视频异常检测中的性能，在UCF-Crime和XD-Violence数据集上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法过于抽象，忽略了定义复杂异常所需的人类-物体交互或动作语义等细粒度信息，需要更精确的提示框架来引导模型推理。

Method: 提出ASK-Hint框架，将提示组织成语义连贯的组别（如暴力、财产犯罪、公共安全），并制定细粒度的指导问题，使模型预测与判别性视觉线索对齐。

Result: 在UCF-Crime和XD-Violence数据集上的广泛实验表明，ASK-Hint持续提升AUC指标，相比现有基准方法达到最先进性能，优于微调和无训练方法。

Conclusion: 研究强调了提示粒度的重要性，确立了ASK-Hint作为可解释视频异常检测的无训练、可泛化解决方案。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify是一个解决2D视觉语言模型特征迁移到3D语义分割中噪声和几何不一致问题的框架，通过学生亲和网络利用3D自监督教师模型的几何先验来净化特征，仅需约1.5%的训练数据就能达到或超越最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的2D到3D特征迁移方法存在权衡：直接投影会产生噪声和碎片化预测，而强制几何一致性需要昂贵的训练流程和大规模标注数据。这种限制源于分割-匹配范式无法协调2D语义与3D几何结构。

Method: 提出GeoPurify框架，使用小型学生亲和网络从3D自监督教师模型中提取几何先验来净化2D VLM生成的3D点特征；在推理时设计几何引导池化模块进一步去噪并确保语义和结构一致性。

Result: 在主要3D基准测试上的广泛实验表明，GeoPurify仅使用约1.5%的训练数据就能达到或超越最先进性能，有效缓解了现有方法的权衡问题。

Conclusion: GeoPurify通过利用潜在几何信息和学习的亲和网络，有效缓解了2D到3D特征迁移中的权衡问题，实现了卓越的数据效率，为3D语义分割提供了一种高效解决方案。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 提出了一种基于猪耳静脉模式的非侵入性生物识别方法，使用智能手机采集图像并通过机器学习实现98.12%的识别准确率，为小规模养殖户提供成本效益高的动物识别方案。


<details>
  <summary>Details</summary>
Motivation: 传统猪只识别方法（如耳标和微芯片）不可靠、成本高且主要针对纯种猪，对小规模养殖户不实用。需要开发一种非侵入性、成本效益高的识别方法。

Method: 收集20头混种猪的800张耳部图像，使用标准智能手机和简单背光拍摄。开发多阶段计算机视觉流程增强静脉可见性，提取结构和空间特征生成生物特征签名，使用支持向量机等机器学习模型进行分类。

Result: 支持向量机模型在混种猪群中达到98.12%的识别精度，从图像处理到分类的整个过程平均耗时8.3秒，适合实时农场部署。

Conclusion: 通过用永久性生物标记替代脆弱的物理标识符，该系统为养殖户提供了成本效益高且无压力的动物识别方法，证实了耳静脉生物识别技术在数字化畜牧管理中的实用性。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 提出了一种多类别计数框架，使用Twins金字塔视觉transformer骨干网络和专门的多类计数头，通过双任务设计和类别聚焦模块减少类别间干扰，在密集和遮挡场景中实现优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 在密集和遮挡场景中，传统的基于检测的计数方法会失效，需要密度图估计方法来准确计数。现有方法在多个类别同时存在时存在类别间干扰问题。

Method: 使用Twins金字塔视觉transformer作为骨干网络，构建专门的多类计数头，采用多尺度解码方法。通过双任务设计添加基于分割的类别聚焦模块，在训练时抑制类别间交叉干扰。

Result: 在VisDrone和iSAID基准测试中表现优于先前的多类别人群计数方法（MAE分别减少33%、43%和64%），与YOLOv11的比较证明了在密集场景中人群计数方法的必要性。

Conclusion: 该方法通过区域损失将多类人群计数扩展到新领域，在生物多样性监测数据集上的应用展示了其支持保护工作和实现可扩展生态洞察的能力。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练或额外监督的方法，通过利用文本到视频扩散模型中的交叉注意力图，实现对生成视频中视觉概念时间对齐的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有的生成视频模型缺乏细粒度的时间控制，无法让用户指定特定视觉元素在生成序列中出现的时间。

Method: TempoControl使用交叉注意力图，通过新颖的优化方法引导概念的时间安排，采用三个互补原则：通过相关性对齐时间形状、通过能量在需要可见性的地方放大注意力、通过熵保持空间焦点。

Result: 该方法能够在确保高质量和多样性的同时，实现对时间安排的精确控制，在多种视频生成应用中表现出有效性。

Conclusion: TempoControl为生成视频模型提供了无需重新训练的时间控制能力，在单对象和多对象的时间重排、动作和音频对齐生成等应用中具有广泛适用性。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 提出了RewardMap框架，通过多阶段强化学习和难度感知奖励设计，解决多模态大语言模型在细粒度视觉推理任务中的稀疏奖励和优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在结构化信息丰富场景（如交通地图）中的细粒度视觉推理能力不足，标准强化学习面临稀疏奖励和优化不稳定的挑战。

Method: 构建ReasonMap-Plus数据集提供密集奖励信号，提出RewardMap多阶段强化学习框架，包含难度感知奖励设计和从简单感知到复杂推理的多阶段训练策略。

Result: 在ReasonMap和ReasonMap-Plus上的实验显示，RewardMap各组件均带来性能提升，组合使用效果最佳，在6个基准测试上平均提升3.47%。

Conclusion: RewardMap框架有效提升了多模态大语言模型的视觉理解和推理能力，特别是在细粒度视觉推理任务中表现优异。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是首个有效利用FLUX强大先验进行拖拽式图像编辑的框架，通过区域级编辑范式和个性化适配器，在拖拽编辑任务中显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型从UNet-based DDPMs转向更可扩展的DiT with flow matching，生成先验变得更强，但拖拽式编辑尚未从中受益。传统点级拖拽编辑在DiT上表现不佳，因为DiT特征结构不足以提供可靠的点级运动监督。

Method: 提出区域级编辑范式，使用仿射变换实现更丰富一致的特征监督；集成预训练开放域个性化适配器增强主体一致性；使用梯度掩码硬约束保护背景保真度；利用多模态大语言模型解决任务歧义。

Result: 在DragBench-DR和ReD Bench上的广泛实验表明，DragFlow超越了基于点和区域的基线方法，在拖拽式图像编辑中创下了新的最先进水平。

Conclusion: DragFlow成功地将FLUX的强大先验应用于拖拽式编辑，通过区域级监督和个性化增强，显著提升了编辑质量，为拖拽式图像编辑开辟了新方向。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 提出F2C方法，通过选择关键片段而非孤立关键帧来提升视频理解，同时采用自适应分辨率策略保持固定计算预算，在多个长视频基准上显著优于均匀采样。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型面临"大海捞针"问题：原始视频帧产生大量视觉标记耗尽模型上下文窗口。现有解决方案通过选择稀疏帧集来减少标记数量，但这种方法丢弃了基本的时间动态信息，导致对运动和事件连续性的推理效果不佳。

Method: 提出F2C方法：1）从孤立关键帧扩展到关键片段选择，关键片段是短而时间连贯的片段；2）采用自适应分辨率策略，动态平衡空间分辨率和片段长度，确保每个视频的标记数量恒定。

Result: 在三个长视频基准上的实验表明，这种无需训练的方法在Video-MME、LongVideoBench和MLVU基准上分别比均匀采样高出8.1%、5.6%和10.3%。

Conclusion: 研究结果强调了在帧选择中保持时间连贯性的重要性，并为将视频LLM扩展到现实世界视频理解应用提供了实用途径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 该研究比较了基于单目视频的3D人体姿态估计模型与惯性测量单元(IMU)在临床相关日常活动中的性能，发现MotionAGFormer表现最佳，两种技术都适用于实验室外的运动学评估，但各有成本、可访问性和精度方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习和可穿戴传感器的发展，在非专业实验室环境下捕捉和分析人体运动，为远程医疗、运动科学和康复提供准确评估。

Method: 使用VIDIMU数据集，包含13种临床相关日常活动，比较四种深度学习框架(MotionAGFormer、MotionBERT、MMPose 2D-to-3D姿态提升和NVIDIA BodyTrack)与基于IMU数据的关节角度计算。

Result: MotionAGFormer表现最优，总体RMSE为9.27°±4.80°，MAE为7.86°±4.18°，Pearson相关系数0.86±0.15，决定系数R²为0.67±0.28。两种技术都适用于实验室外运动学评估。

Conclusion: 研究阐明了现成视频模型在健康成年人中提供临床有前景的运动学数据的情况，以及与IMU估计相比的差距，为开发稳健、成本效益高且用户友好的远程医疗解决方案提供了指导方针。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift通过整合AutoKL和CLIP适配器的扩散模型，实现了从fMRI数据中准确重建视觉刺激的跨被试方法，仅需训练17%参数即可达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决从脑活动中重建视觉信息时面临的跨被试差异性和计算需求大的挑战，特别是神经表征的个体间变异性和大脑对复杂视觉输入的抽象语义编码问题。

Method: 提出NeuroSwift方法，通过扩散模型整合互补适配器：AutoKL处理低级特征，CLIP处理语义特征。CLIP适配器在Stable Diffusion生成图像和COCO字幕对上训练，模拟高级视觉皮层编码。跨被试泛化时，先在一个被试上预训练，然后仅微调17%参数（全连接层）适应新被试。

Result: 在轻量级GPU（三块RTX 4090）上，每个被试仅需1小时训练即可达到最先进性能，且优于现有方法。

Conclusion: NeuroSwift提供了一种高效且准确的跨被试视觉刺激重建方法，通过参数高效微调和互补特征适配器解决了神经解码中的关键挑战。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP是一个自训练框架，通过联合优化CLIP的视觉和文本表示来提升细粒度图像分类性能，核心是TokenFusion模块中的SOAP机制和动态知识聚合。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在细粒度分类任务中表现受限，因为它依赖粗粒度的全局特征，而忽略了空间精度。现有方法通过LLM描述对齐CLS token，但缺乏空间精确性。

Method: 提出Saliency-Oriented Attention Pooling (SOAP)构建细粒度token，与全局CLS token融合；使用双头LLM分类器（冻结和可学习）提供稳定文本先验；动态知识聚合结合固定先验和演化logits迭代优化伪标签。

Result: 在13个细粒度基准测试中平均准确率提升2.90%，仅需轻量级适配。

Conclusion: microCLIP能够有效挖掘CLIP中的潜在细粒度信号，显著提升细粒度分类性能，同时保持轻量适配。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1是首个通过多模态大语言模型和群体相对策略优化实现的高精度视频真实性检测器，能提供可解释的检测结果。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频技术的快速发展，需要有效的检测工具来应对错误信息和声誉损害等社会风险，同时检测模型需要提供可解释的结果以确保透明度。

Method: 使用群体相对策略优化(GRPO)微调多模态大语言模型(MLLM)，构建包含14万真实和AI生成视频的挑战性数据集，并设计专门针对时间伪影和生成复杂度的奖励模型。

Result: VidGuard-R1在现有基准测试中实现了最先进的零样本性能，额外训练后准确率超过95%，并能产生精确且可解释的预测理由。

Conclusion: VidGuard-R1通过结合高精度检测和可解释性，为AI生成视频检测提供了有效的解决方案，代码已公开可用。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出了一种简单有效的方法来缓解长视频生成中的质量退化问题，无需长视频教师监督或重新训练长视频数据集，通过利用教师模型的丰富知识为学生模型提供指导。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了突破性进展，但基于transformer架构的计算成本过高，特别是在生成长视频时。现有的自回归方法在超出训练范围时会出现明显的质量退化问题。

Method: 利用教师模型的丰富知识，通过从自生成长视频中抽取片段为学生模型提供指导。该方法保持时间一致性，可将视频长度扩展到教师能力的20倍，避免了过曝光和错误累积等问题。

Result: 能够生成长达4分15秒的视频，相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50多倍。在标准基准测试和改进基准测试中，该方法在保真度和一致性方面显著优于基线方法。

Conclusion: 该方法有效解决了长视频生成中的质量退化问题，无需额外的长视频监督或重新训练，在保持时间一致性的同时显著扩展了视频生成长度。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一个基于物理引导的视频生成方法，通过两阶段训练策略和对象掩码，实现了对刚体运动、交互和效果的逼真控制。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在物理合理的对象交互方面存在不足，缺乏基于物理的控制机制，限制了其在机器人学和具身决策中的应用潜力。

Method: 提出两阶段训练策略，通过对象掩码逐步移除未来运动监督，在合成场景上训练视频扩散模型，并结合低级运动控制与高级文本条件。

Result: 在真实场景中显著改善了对象交互效果，在复杂动态现象合成方面表现出色，相比同类规模模型有明显提升。

Conclusion: KineMask通过物理引导的视频生成方法，成功实现了逼真的刚体控制和交互效果，证明了低级和高级条件在视频扩散模型中的互补作用。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 提出了一种多模态细粒度动作模拟方法，通过整合本体感觉、动觉、力触觉和肌肉激活等多种感官，提升家用机器人的精细运动控制能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型缺乏细粒度控制，无法作为世界模型。通用家用机器人需要实时精细运动控制来处理精细任务和紧急情况。

Method: 开发了特征学习范式来对齐多模态信息，同时保留每个模态的独特信息；提出正则化方案增强动作轨迹特征在复杂交互动态中的因果性。

Result: 实验表明，整合多模态感官提高了模拟精度，减少了时间漂移；广泛的消融研究和下游应用证明了方法的有效性和实用性。

Conclusion: 多模态细粒度动作模拟能够有效提升机器人精细控制能力，为通用家用机器人提供了更可靠的世界模型基础。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA通过原生稀疏注意力机制增强视频语言模型，在216K视频指令数据集上训练Qwen2.5-VL，实现了对长视频的更好理解和128K tokens的可靠扩展。


<details>
  <summary>Details</summary>
Motivation: 解决多模态语言模型在视频理解中因上下文长度限制而错过关键过渡帧、难以保持长时间连贯性的问题。

Method: 采用硬件感知的混合注意力方法：文本使用密集注意力，视频使用原生稀疏注意力(NSA)，在216K视频指令数据集上进行端到端训练。

Result: 相比token压缩和训练无关的稀疏基线，VideoNSA在长视频理解、时间推理和空间基准测试中表现更优，可靠扩展到128K tokens。

Conclusion: VideoNSA通过可学习的组合稀疏注意力诱导动态注意力汇聚点，在固定预算下实现最优的全局-局部注意力分配，且分支使用模式具有任务依赖性。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift是一种无需训练的方法，通过重新校准基于分辨率大小的去噪器噪声水平，解决了扩散模型在不同分辨率下生成质量不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率文本到图像生成器无法为不需要高分辨率图像的用户提供经济高效的替代方案，因为模型在不同分辨率下的生成质量存在差异。

Method: 提出NoiseShift方法，重新校准去噪器的噪声水平，使其适应不同分辨率，无需改变模型架构或采样计划。

Result: 在LAION-COCO数据集上，NoiseShift将SD3.5、SD3和Flux-Dev的FID分别提高了15.89%、8.56%和2.44%；在CelebA数据集上，分别提高了10.36%、5.19%和3.02%。

Conclusion: NoiseShift有效缓解了分辨率相关的伪影，显著提升了低分辨率图像生成的质量。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 该论文研究从视频预测动态物理属性的任务，包括弹性、粘度和动态摩擦，通过收集新数据集并比较三种推断方法：基于传统视觉技术的oracle方法、使用视觉提示的预训练视频模型、以及多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 研究如何从视频中推断需要时间信息才能判断的动态物理属性，如弹性、粘度和动态摩擦，以解决传统方法难以直接从视频中提取这些复杂物理特性的问题。

Method: 收集了三个物理属性的新视频数据集（合成训练/测试集和真实世界集）；探索了三种推断方法：(a) 使用传统计算机视觉技术的oracle方法，(b) 基于预训练视频生成和自监督模型的视觉提示机制，(c) 多模态大语言模型的提示策略。

Result: 生成式或自监督预训练的视频基础模型表现相似，但落后于oracle方法；多模态大语言模型目前表现较差，但通过合适的提示可以改善性能。

Conclusion: 视频基础模型在动态物理属性预测任务上具有潜力，但仍有改进空间；多模态大语言模型需要更好的提示策略来提升性能。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 提出声音对象检测任务，通过多模态对象感知框架从第一人称视角视频中学习，使用自动分割管道和槽注意力视觉编码器来识别产生声音的物体。


<details>
  <summary>Details</summary>
Motivation: 日常物体交互会产生独特的声音，但现有模型难以将声音与直接涉及的物体联系起来。受人类感知启发，需要开发能够识别声音来源物体的模型。

Method: 开发自动分割管道计算物体分割掩码，使用槽注意力视觉编码器强化对象先验，从第一人称视角视频中学习多模态对象感知框架。

Result: 在新任务上达到最先进性能，并在现有多模态动作理解任务上表现优异。

Conclusion: 提出的对象感知框架能够有效识别声音来源物体，在声音对象检测任务中表现出色。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 本文分析了3D高斯泼溅(3DGS)对图像级投毒攻击的脆弱性，提出了一种基于密度的投毒方法，通过向低密度区域注入高斯点来嵌入视角依赖的幻觉物体，并引入自适应噪声策略破坏多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 随着3D场景表示方法如NeRF和3DGS在新视角合成中的广泛应用，解决这些方法的脆弱性变得至关重要。本文旨在分析3DGS对图像级投毒攻击的鲁棒性。

Method: 提出密度引导的投毒方法，使用核密度估计识别低密度区域，战略性地注入高斯点来嵌入视角依赖的幻觉物体。同时引入自适应噪声策略来破坏多视角一致性。

Result: 广泛的实验表明，该方法在性能上优于现有最先进技术，能够从投毒视角清晰显示幻觉物体，同时对无辜视角影响最小。

Conclusion: 本文不仅提出了一种有效的3DGS投毒攻击方法，还引入了基于KDE的评估协议，为未来研究提供了客观的基准测试框架。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 提出了首个理论框架和两种架构无关的算法来解决文本到图像模型在多主体生成中的属性泄漏、身份纠缠和主体遗漏问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在单主体提示上表现优秀，但在多主体描述中经常出现属性泄漏、身份纠缠和主体遗漏等问题，需要专门的方法来提升多主体保真度。

Method: 通过将流匹配视为随机最优控制问题，提出了两种算法：(1)无需训练、测试时控制的单次更新方法；(2)轻量级微调规则Adjoint Matching，同时保持基础模型能力。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL上的实验表明，两种算法都能持续改善多主体对齐，同时保持基础模型风格。测试时控制可在普通GPU上高效运行，微调控制器在有限提示上训练后能泛化到未见提示。

Conclusion: 提出的FOCUS方法在多个模型上实现了最先进的多主体保真度，为多主体文本到图像生成提供了有效的解决方案。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>
